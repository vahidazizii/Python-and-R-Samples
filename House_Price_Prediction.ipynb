{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import reqired libraries\n",
    "###########################\n",
    "\n",
    "# General Librarys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Scikit-learn library models, metric, etc\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso,ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Boosting methods (XGBosst, lightGBM)\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "# import joblib to save and load prediction models\n",
    "import joblib\n",
    "\n",
    "#suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load train and test dataset\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for year and month in training data\n",
    "df_train['date'] = pd.to_datetime(df_train['date'].astype(str), format='%Y%m%d')\n",
    "df_train['year'] = df_train['date'].dt.year\n",
    "df_train['month'] = df_train['date'].dt.month\n",
    "\n",
    "# Create new columns for year and month in test data\n",
    "df_test['date'] = pd.to_datetime(df_test['date'].astype(str), format='%Y%m%d')\n",
    "df_test['year'] = df_test['date'].dt.year\n",
    "df_test['month'] = df_test['date'].dt.month\n",
    "\n",
    "# Delete the old date columns in train and test data\n",
    "df_train = df_train.drop(['date'], axis = 1)\n",
    "df_test = df_test.drop(['date'], axis = 1)\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coralation plot (heatmap) and save it\n",
    "corr = df_train.corr()\n",
    "plt.subplots(figsize=(20,9))\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmap with abs(corr) values grater than 0.2 and save it\n",
    "top_feature = corr.index[abs(corr['price'])>0.2]\n",
    "\n",
    "plt.subplots(figsize=(12, 8))\n",
    "top_corr = df_train[top_feature].corr()\n",
    "sns.heatmap(top_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot houses' geographic coordinates based on their grade\n",
    "sns.lmplot('lat', 'long', data=df_train, hue='grade', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scatter plot, price against number of bedroom \n",
    "# 1. helps to find the the relationship between pricec and number of bedrooms\n",
    "# 2. helps to identify outliers\n",
    "sns.lmplot(x='bedrooms',y='price',data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot, price against number of bedroom colored with grades\n",
    "sns.lmplot(x='bedrooms',y='price',data=df_train,hue='grade',fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot, price against sqft_living colored with grades\n",
    "\n",
    "sns.lmplot(x='sqft_living',y='price',data=df_train,hue='grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_location):\n",
    "    ''' Load train and test data, then:\n",
    "    1. Drop \"property\" column since it is a unique identifier\n",
    "    2. Add new columns for year and month \n",
    "    3. Delete date column from dataset\n",
    "    4. Create new features for dataset by calculating distance of each house from downtown\n",
    "    \n",
    "    Parameters:\n",
    "        data_location(str): data location in local machine\n",
    "        down_lat(float): down town latitude value\n",
    "        down_long(float): down town langitude value\n",
    "    Returns:\n",
    "        df(pandas dataframe): preprocessed pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(data_location)\n",
    "\n",
    "    # Delete property\n",
    "    df = df.drop(['property'], axis = 1)\n",
    "\n",
    "    # Add month and year as new columns and delete the date column\n",
    "    df['date'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d')\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "\n",
    "    # Delete the old date column\n",
    "    df = df.drop(['date'], axis = 1)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform following columns to meake them less skewd and closer to normal distribution\n",
    "def log_transform(df, trans_columns):\n",
    "    ''' Apply log transform to meake columns less skewd and closer to normal distribution\n",
    "    \n",
    "    Parameters:\n",
    "        df(pandas dataframe): raw dataframe \n",
    "        trans_columns(array): list of columns needs log transform      \n",
    "    \n",
    "    Returns:\n",
    "        df_updated(pandas dataframe): Updated dataframe with tranformed columns columns\n",
    "    '''\n",
    "    \n",
    "    for column in trans_columns:\n",
    "        df[column] = np.log(1 + df[column])\n",
    "    df_updated = df\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(df, down_lat, down_long):\n",
    "    ''' Generate new features\n",
    "    \n",
    "    Parameters:\n",
    "        df(pandas dataframe): raw dataframe \n",
    "        down_lat(float): downtown latitude, used to generate a new feature        \n",
    "        down_long(float): downtown longitude, used to generate a new feature \n",
    "    \n",
    "    Returns:\n",
    "        df_updated(pandas dataframe): Updated dataframe with new columns\n",
    "    '''\n",
    "    \n",
    "    # Create new columns representing distance of each house from downtown\n",
    "    df['lat_to_downtown']=(df['lat']-down_lat).abs()\n",
    "    df['long_to_downtown']=(df['long']-(down_long)).abs()\n",
    "    \n",
    "    # Create new column representing mean sqft of lot for each house in each (zipcode,grade) group\n",
    "    df['mean_sqft_lot'] = df.groupby(by=['zipcode','grade'])['sqft_lot'].transform('mean')\n",
    "    \n",
    "    # Create new column representing mean sqft of living areain 2015 for each house in each (zipcode,grade) group\n",
    "    df['mean_sqft_living15'] = df.groupby(by=['zipcode','grade'])['sqft_living15'].transform('mean')\n",
    "    \n",
    "    # Create new column representing mean sqft of lot area in 2015 for each house in each (zipcode,grade) group\n",
    "    df['mean_sqft_lot15'] = df.groupby(by=['zipcode','grade'])['sqft_lot15'].transform('mean')\n",
    "    \n",
    "    # Create new column representing mean sqft of above area for each house in each (zipcode,grade) group\n",
    "    df['mean_sqft_sqft_above'] = df.groupby(by=['zipcode','grade'])['sqft_above'].transform('mean')\n",
    "    \n",
    "    # Create new column representing mean grade for each house in each zipcode group\n",
    "    df['mean_grade'] = df.groupby(by=['zipcode'])['grade'].transform('mean')\n",
    "    \n",
    "    df_updated = df\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train and test datasets\n",
    "df_train = preprocessing('train.csv')\n",
    "\n",
    "# Use log transform to make the columns less skewed (to meet the assumptions of inferential statistics)\n",
    "trans_columns = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_lot15']\n",
    "df_train = log_transform(df_train, trans_columns + ['price'])\n",
    "\n",
    "# Preprocess train and test datasets\n",
    "df_train = feature_generator(df_train, 47.36217, -122.20069)\n",
    "\n",
    "# Additional step in preprocessing train data \n",
    "# Delete outliers in number of bedrooms from train data\n",
    "df_train=df_train[df_train['bedrooms']<11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset to train data (90% of data) and validation data (10 % of data)\n",
    "X_train,X_valid,y_train,y_valid=train_test_split(df_train.drop('price',axis=1),df_train['price'],test_size=0.1, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 adjusted function used to evaluate performance of LightGBM and Xgboost models\n",
    "def adjustedR2(r2,n,k):\n",
    "    return r2-(k-1)/(n-k)*(1-r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgbm(X_train, y_train, X_valid, y_valid):\n",
    "    ''' Train LightGBM model and save it\n",
    "    \n",
    "    Parameters:\n",
    "        X_train(pandas dataframe): Training data\n",
    "        y_train(array): Response variable of training data         \n",
    "        X_valid(pandas dataframe): Validation data\n",
    "        y_valid(array): Response variable of validation data \n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # Create train and validation datasets for LightGBM classifier\n",
    "    lgbm_train = lgbm.Dataset(X_train, y_train)\n",
    "    lgbm_eval = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train)\n",
    "\n",
    "    # Create grid parameters for tuning LightGBM\n",
    "    gridParams = {\n",
    "        \"max_depth\": [-1],\n",
    "        'learning_rate': [0.04,0.045,0.05,0.055],\n",
    "        'n_estimators': [200,500],\n",
    "        'num_leaves': [20,25,50],\n",
    "        'boosting_type' : ['gbdt'],\n",
    "        'feature_fraction': [0.7,0.9],\n",
    "        'bagging_fraction' :[0.9],\n",
    "        'bagging_freq' :[10]  \n",
    "        }\n",
    "\n",
    "    # Define the fixed parameters dictionary\n",
    "    params = {'random_state' : 10,\n",
    "              'objective': 'regression',\n",
    "              'min_data_in_leaf': 20,      \n",
    "              'metric': [12,'rmse']\n",
    "             }\n",
    "\n",
    "    # Create regressor \n",
    "    clf = lgbm.LGBMRegressor(**params)\n",
    "\n",
    "    # Create the grid\n",
    "    grid = GridSearchCV(clf, gridParams,\n",
    "                        verbose=2,\n",
    "                        cv=5,\n",
    "                        n_jobs=2,\n",
    "                        return_train_score = True)\n",
    "    # Run the grid\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Select the best values obtained in tuning part\n",
    "    params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "    params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "    params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "    params['max_depth']= grid.best_params_['max_depth']\n",
    "    params['learning_rate']= grid.best_params_['learning_rate']\n",
    "    params['num_leaves']= grid.best_params_['num_leaves']\n",
    "    params['boosting_type']= grid.best_params_['boosting_type']\n",
    "    params['bagging_fraction']= grid.best_params_['bagging_fraction']\n",
    "    params['bagging_freq']= grid.best_params_['bagging_freq']\n",
    "    params['feature_fraction']= grid.best_params_['feature_fraction']\n",
    "\n",
    "    # Train with tuned parameters\n",
    "    evaluation_results = {}\n",
    "    clf_tuned = lgbm.train(params,\n",
    "                     lgbm_train,\n",
    "                     valid_sets=[lgbm_train,lgbm_eval], \n",
    "                     valid_names=['Train', 'Test'],\n",
    "                     categorical_feature=[13],\n",
    "                     evals_result=evaluation_results,\n",
    "                     num_boost_round=1500,\n",
    "                     early_stopping_rounds=300,\n",
    "                     verbose_eval=20\n",
    "                    )\n",
    "\n",
    "    # Save the best model \n",
    "    clf_tuned.save_model('lgbm_model.txt', num_iteration=clf_tuned.best_iteration) \n",
    "    \n",
    "    # Load the saved LightGBM model\n",
    "    lgbm_model = lgbm.Booster(model_file='lgbm_model.txt')\n",
    "    \n",
    "    # Predict response varibale valid data by Xgboost\n",
    "    lgbm_preds=lgbm_model.predict(X_valid)\n",
    "    \n",
    "    # Print performance of LightGBM on validation data (MAE, RMSE, R2, and R2_Adjusted values)    \n",
    "    print('Mean absolute error = \\t {}'.format(np.round(mean_absolute_error(y_valid, lgbm_preds), 2 )))\n",
    "    print('RMSE = \\t {}'.format(np.round(np.sqrt(mean_squared_error(y_valid, lgbm_preds)),5 )))\n",
    "    print('R2 = \\t {}'.format(np.round(r2_score(y_valid, lgbm_preds), 3 )))\n",
    "    print('R2_adjusted = \\t {}'.format(np.round(adjustedR2(r2_score(y_valid, lgbm_preds),X_valid.shape[0],X_valid.shape[1]), 3)))\n",
    "    \n",
    "    return lgbm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgbm(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_xgb(X_train, y_train, X_valid, y_valid):\n",
    "    ''' Train Xgboost model and save it\n",
    "    \n",
    "    Parameters:\n",
    "        X_train(pandas dataframe): Training data\n",
    "        y_train(array): Response variable of training data         \n",
    "        X_valid(pandas dataframe): Validation data\n",
    "        y_valid(array): Response variable of validation data \n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Create grid parameters for Xgboost\n",
    "    param_grid = {\n",
    "        \"xgbrg__n_estimators\": [500],\n",
    "        \"xgbrg__learning_rate\": [0.17,0.15],\n",
    "        'max_depth': [3,4,5],\n",
    "        'subsample': [0.75,0.8,0.85]\n",
    "    }\n",
    "\n",
    "    # Define the fixed parameters dictionary\n",
    "    fit_params = {\"xgbrg__eval_set\": [(X_valid, y_valid)], \n",
    "                  \"xgbrg__early_stopping_rounds\": 50, \n",
    "                  \"xgbrg__verbose\": False,\n",
    "                  \"tree_method \": \"hist\",\n",
    "                  \"grow_policy\" : \"lossguide\"} \n",
    "\n",
    "    # Create the grid and run it\n",
    "    xgbr = xgb.XGBRegressor(**fit_params)\n",
    "    searchCV = GridSearchCV(xgbr, cv=10, param_grid=param_grid,verbose=2, n_jobs=2,scoring= 'neg_mean_squared_error')\n",
    "    searchCV.fit(X_train, y_train) \n",
    "\n",
    "    # Training Xgboost with the cv winner\n",
    "    xgb_pars={}\n",
    "    xgb_pars={**fit_params,**searchCV.best_params_}\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "    eval_set = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "    num_round=500\n",
    "    xgb_tuned = xgb.train(xgb_pars, dtrain , num_round, eval_set , early_stopping_rounds=250,verbose_eval=15)\n",
    "\n",
    "    # Save Xgboost model\n",
    "    joblib.dump(xgb_tuned,'xgb_model') \n",
    "    \n",
    "    #load saved model\n",
    "    xgb_tuned = joblib.load('xgb_model')\n",
    "    best_iteration = xgb_tuned.best_iteration\n",
    "\n",
    "    # Predict response varibale valid data by Xgboost\n",
    "    xgb_preds = xgb_tuned.predict(dvalid, ntree_limit = best_iteration)\n",
    "\n",
    "    # Print performance of LightGBM on validation data (MAE, RMSE, R2, and R2_Adjusted values)\n",
    "    print('Mean absolute error = \\t {}'.format(np.round(mean_absolute_error(y_valid, xgb_preds), 2 )))\n",
    "    print('RMSE = \\t {}'.format(np.round(np.sqrt(mean_squared_error(y_valid, xgb_preds)),5 )))\n",
    "    print('R2 = \\t {}'.format(np.round(r2_score(y_valid, xgb_preds), 3 )))\n",
    "    print('R2_adjusted = \\t {}'.format(np.round(adjustedR2(r2_score(y_valid, xgb_preds),X_valid.shape[0],X_valid.shape[1]), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_xgb(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_test_predictor(model_name, data):\n",
    "    ''' Load LightGBM saved model and predict test data\n",
    "    \n",
    "    Parameters:\n",
    "        model_name(str): LightGBM saved model's name\n",
    "        data(pandas dataframe): Test data \n",
    "    \n",
    "    Returns:\n",
    "        LightGBM_test_preds(array): Predicted values for test data\n",
    "    '''\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    lgbm_model = lgbm.Booster(model_file='lgbm_model.txt')\n",
    "    \n",
    "    # Drop identifier column from test data \n",
    "    df_test_lgbm = df_test.drop('id',axis=1)\n",
    "    \n",
    "    # Predict test dataset by tuned LightGBM and save the data to be passed to ensemble model\n",
    "    lgbm_test_preds = lgbm_model.predict(df_test_lgbm)\n",
    "\n",
    "    return lgbm_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_test_predictor(model_name, data):\n",
    "    ''' Load Xgboost saved model and predict test data\n",
    "    \n",
    "    Parameters:\n",
    "        model_name(str): Xgboost saved model's name\n",
    "        data(pandas dataframe): Test data \n",
    "    \n",
    "    Returns:\n",
    "        XGB_test_preds(array): Predicted values for test data\n",
    "    '''\n",
    "\n",
    "    # Load Xgboost model \n",
    "    xgb_model = joblib.load('xgb_model')\n",
    "    \n",
    "    # Drop identifier column from test data\n",
    "    df_test_xgb =df_test.drop('id',axis=1)\n",
    "    df_test_xgb_DM=xgb.DMatrix(df_test_xgb)\n",
    "    \n",
    "    # Predict test dataset by tuned Xgboost and save the data to be passed to ensemble model\n",
    "    xgb_test_preds = xgb_model.predict(df_test_xgb_DM)\n",
    "\n",
    "    return xgb_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess train and test datasets\n",
    "df_test = preprocessing('test.csv')\n",
    "\n",
    "# Use log transform to make the columns less skewed (to meet the assumptions of inferential statistics)\n",
    "trans_columns = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_lot15']\n",
    "df_test = log_transform(df_test, trans_columns)\n",
    "\n",
    "# Preprocess test datasets\n",
    "df_test = feature_generator(df_test, 47.36217, -122.20069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data by LightGBM and Xgboost models\n",
    "lgbm_test_preds = lgbm_test_predictor('lgbm_model.txt', df_test)\n",
    "xgb_test_preds = xgb_test_predictor('xgb_model', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
